{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d39d71a",
   "metadata": {},
   "source": [
    "# Multi-Layer-Perceptron (MLP) to recognize handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a42f3d",
   "metadata": {},
   "source": [
    "In this lab, you will implement a simple MLP to recognize handwritten digits. We will use the MNIST dataset as our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12f0997",
   "metadata": {},
   "source": [
    "## Setup the necessary imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c82ac45",
   "metadata": {},
   "source": [
    "The MNIST dataset is a dataset of handwritten digits. It is a widely used dataset for benchmarking methods (although over the past years it has become \"too easy\"). It is a good starting point to learn how MLPs and CNN work. \n",
    "\n",
    "The goal today is to implement a model that will predict the value of a handwritten digit, represented by an 8x8 image. Your MLP should be able to take as input the 64 pixels of that image, and output a value between 0 and 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b6ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Installation of the necessary libraries\n",
    "!pip install numpy matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99de6dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86043fb4",
   "metadata": {},
   "source": [
    "## The MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecde43ea",
   "metadata": {},
   "source": [
    "The MNIST dataset is a collection of images with a label associated. scikit-learn provides a subset of this dataset where the images have been downsized to be 8x8. We will use the function `load_digits` to load the data. The following cells will load the data and present the important properties of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6282f57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the dataset using the function load_digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9938922f",
   "metadata": {},
   "source": [
    "The variable `digits` now contains all the information that we need to train our model. The two attributes of interest are the `data` and `target` attributes. Let's visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff19587",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = digits.data.shape\n",
    "print(f\"The MNIST dataset has {n_samples} examples, and each image has {n_features} features (pixels)\")\n",
    "print(f\"The data is represented in a {type(digits.data)}\")\n",
    "print(f\"You access the trainin label with digits.target : {type(digits.target)}, for example the first sample is a {digits.target[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01914e6c",
   "metadata": {},
   "source": [
    "Let's also visualize the image, run the cell below to see the digits and their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f2b775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation of the digits data set\n",
    "import matplotlib.pyplot as plt\n",
    "# set up the figure\n",
    "fig = plt.figure(figsize=(6, 6))  # figure size in inches\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "# plot the digits: each image is 8x8 pixels\n",
    "for i in range(64):\n",
    "    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')\n",
    "    \n",
    "    # label the image with the target value\n",
    "    ax.text(0, 7, str(digits.target[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a8884e",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e647b054",
   "metadata": {},
   "source": [
    "In Machine Learning, preprocessing is an important step to avoid multiple issues. In many datasets, images come in different shape, scale and color format. Preprocessing ensures that the images are \"cleaned\" of any irregularities and are ready to be processed by the model, which can now focus on the task at hand using consistent input. \n",
    "\n",
    "In our case, the images have already undergone a lot of preprocessing : they have been resized to be the same shape and are all grayscale. However they still need to be `normalized`.\n",
    "\n",
    "`Normalization` is an important step to creating a consistent scale accross images. The value of a pixel can vary greatly (here it's between 0 and 16). These high, unnormalized values can make learning difficult because certain values or patterns may dominate the model's training. Normalization usually scales pixel values to a range between 0 and 1 (or -1 and 1), making it easier for the model to process them in a balanced way.\n",
    "It will also avoid issues with gradients. As values are large and unnormalized, the gradients can be large or uneven, making the gradient descent algorithm having unstable updates.\n",
    "\n",
    "In our case, since the pixel values are between 0 and 16, we will normalize them to be between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7fa936",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check the minimum and maximum value of a pixel in the dataset\n",
    "print(f\"Pixel values are between {np.min(digits.data)} and {np.max(digits.data)}\")\n",
    "\n",
    "processed_data = digits.data / 16\n",
    "\n",
    "print(f\"After normalization, pixel values are between {np.min(processed_data)} and {np.max(processed_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e9dbac",
   "metadata": {},
   "source": [
    "### Label encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4513bafd",
   "metadata": {},
   "source": [
    "Our model will predict the digit associated with each image, ie it needs to predict integers between 0 and 9. For that, at each input, it will predict a value for each of the possible digits. The value will represent its confidence in the prediction. We then need to compare this 10 dimensional vector with the true label. However, our label is just a single digit. We need to encode it as a vector with 10 dimensions too. This is the purpose of `one hot` encoding.\n",
    "\n",
    "`One hot`encoding converts a label $i$ into a $n$-dimensional vector $e^{(i)}$ where $\\forall j \\neq i, e_j^{(i)} = 0$ and $e_i^{(i)} = 1$\n",
    "\n",
    "For our case since we have 10 labels, for example the one-hot encoding of label $0$ is $(1,0,0,0,0,0,0,0,0,0)$ and label 3 is $(0,0,0,1,0,0,0,0,0,0)$\n",
    "\n",
    "This is a better representation for a machine, and also has the nice property to be a probability. Which will be useful for our `cross entropy`loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4057bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : implement one hot encoding function\n",
    "def one_hot(n_classes, y):\n",
    "    return 0\n",
    "\n",
    "## TODO : test the encoding\n",
    "print(one_hot(10, 0)) ## should return (1,0,0,0,0,0,0,0,0,0)\n",
    "print(one_hot(10, 3)) ## should return (0,0,0,1,0,0,0,0,0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761f11df",
   "metadata": {},
   "source": [
    "### Train-test split "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c73cd14",
   "metadata": {},
   "source": [
    "We want our model to achieve good [generalization](https://en.wikipedia.org/wiki/Bias–variance_tradeoff). This means that the model should perform well on unseen data. We thus need to split our dataset into a train set and a test set. The train set will be used for training the model, and we will evaluate its performances on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8833eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : implement the train test split\n",
    "def train_test_split(data, targets):\n",
    "    train_set = None\n",
    "    test_set = None \n",
    "\n",
    "    return train_set, test_set\n",
    "\n",
    "train_set, test_set = train_test_split(processed_data, digits.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d47e14",
   "metadata": {},
   "source": [
    "We have now successfully loaded and preprocessed the dataset. The data is ready to be input to the model. Our goal is to implement a Multi-Layer-Perceptron (MLP) to process the images and predict the digit it represent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b306b767",
   "metadata": {},
   "source": [
    "## Implementation of the multi-layer perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8d1f8e",
   "metadata": {},
   "source": [
    "We will now implement the MLP. An MLP is a series of layers of perceptrons. A perceptron layer is a stack of perceptrons that will take the same input and each produce their individual output by linear combination of the input. Then the outputs will be passed through the activation function. Then the next layer will take that output as their input. We provide an indicative structure to implement the MLP, but you can come up with your own.\n",
    "\n",
    "More details about MLPs can be found in this [book](http://neuralnetworksanddeeplearning.com/) and in the course material.\n",
    "\n",
    "We give here precision about the forward and backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a8f4ff",
   "metadata": {},
   "source": [
    "### Forward pass "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ccd645",
   "metadata": {},
   "source": [
    "In the forward pass, the input is processed by the model to make a prediction about its label. The Linear layer performs the computation : \n",
    "$$ z = W x + b $$\n",
    "Where $W$ is the weight matrix, $b$ is the bias vector.\n",
    "This output is then pass to the activation layer that performs :\n",
    "$$ y = \\sigma(z) $$ \n",
    "Where $\\sigma$ is the sigmoid activation function.\n",
    "These are the only two layers that we will use with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248f6ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : we provide an indicative API for implementing the layers and the MLP\n",
    "## you can follow it or come up with you own\n",
    "\n",
    "\n",
    "## TODO : implement sigmoid activation function \n",
    "def sigmoid(x):\n",
    "    return 0\n",
    "\n",
    "class SigmoidLayer : \n",
    "    \"\"\"This class is a sigmoid layer, it takes as input the output of the linear layer and compute the sigmoid of it\"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Initialization of the module\n",
    "        TODO : implement necessary initialization\"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Computes the forward pass through this layer.\n",
    "        TODO : implement\"\"\"\n",
    "        return x\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"This is to ensure we can call it like a function\"\"\"\n",
    "        return self.forward(x)\n",
    "         \n",
    "## TODO : implement the linear layer\n",
    "class LinearLayer :\n",
    "    \"\"\"This implements one layer that stacks multiple perceptrons and process a vector input to produce a vector output.\n",
    "       It only performs the linear part.\"\"\"\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        \"\"\"Initialization of the module\n",
    "        TODO : implement necessary initialization\"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Computes the forward pass through this layer : z = Wx + b\n",
    "        TODO : implement\"\"\"\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def __call__(self, x) :\n",
    "        \"\"\"This is to ensure we can call it like a function\"\"\"\n",
    "        return self.forward(x)\n",
    "\n",
    "\n",
    "## TODO : implement the MLP : it should stack linear layers\n",
    "class MLP :\n",
    "    \"\"\"This implements the MLP, it shoud take as input the description of the layers.\n",
    "       For example layers=[5,6,8] defines a MLP that is 3 layers deep where the first layer \n",
    "       has 5 units, the second 6 and the third 8.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_channel, layers):\n",
    "        \"\"\"Initialization of the module\n",
    "        TODO : implement necessary initialization\"\"\"\n",
    "        pass\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \"\"\"This is the forward pass, to compute the output y_pred given the input. It should pass through the layers of the model\"\"\"\n",
    "        return x\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"This is to ensure we can call it like a function\"\"\"\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac819156",
   "metadata": {},
   "source": [
    "Now that our MLP has been implemented, we need to instantiate and run it on our data.\n",
    "For our specific problem, we have images of size 8x8 so we should be able to take an input of size 64.\n",
    "The labels are between 0 and 9 so our model should have 10 outputs, which will be the prediction of each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83055de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : run this cell to instantiate your MLP and run it on the data\n",
    "input_channel = 64\n",
    "layers = [20, 10] ## try different values for the layer\n",
    "\n",
    "model = MLP(input_channel, layers)\n",
    "\n",
    "test_example = processed_data[0]\n",
    "test_label = one_hot(10, digits.target[0])\n",
    "\n",
    "prediction = model(test_example)\n",
    "\n",
    "print(f\"Our model predicted the following output : {prediction}\")\n",
    "print(f\"The true value is : {test_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb36ca7",
   "metadata": {},
   "source": [
    "You will notice that your model outputs rubish data, this is because it has not been trained. We first need a way to measure the quality of the prediction, this is done using a loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22573b6",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "This is a classification problem, so we will use the widely used `cross entropy` function to supervise our model. The cross entropy is defined as follows : \n",
    "\n",
    "$$ L(\\hat{y}, y) = \\sum_{c=1}^C - y_c log(\\hat{y_c}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8534f0d7",
   "metadata": {},
   "source": [
    "The cross entropy function requires the inputs to be probablity distributions, ie : \n",
    "$$ \\sum_{c=1}^C y_c = 1 $$\n",
    "and \n",
    "$$ \\sum_{c=1}^C \\hat{y_c} = 1 $$\n",
    "\n",
    "Our labels have been converted into one-hot encoding and already satisfy this property. However, our model predictions $\\hat{y_c}$ has no restriction. We could enforce the restrictions $\\forall c, 0 \\leq y_c \\leq 1$ and $ \\sum_{c=1}^C \\hat{y_c} = 1 $ in our model architecture, but this would be ill-advised. It is much better to have no restriction on the model output, and then post-process the output to be a probability distribution. Forcing the model output to be a probability distribution would lead to a lot of complication in the gradient calculation. \n",
    "\n",
    "To convert our model output into a probability distribution, we will use the `softmax` function : \n",
    "$$ s_i = softmax(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}} $$\n",
    "\n",
    "The softmax function convert each output $ z_i $ of the neural network into a value between $0$ and $1$, and also has the property that $\\sum s_i = 1$ which is perfect for our cross-entropy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02df54ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : implement the softmax function\n",
    "def softmax(x) :\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3337d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : implement the cross entropy loss function\n",
    "def cross_entropy(y_pred, y_true):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c5babd",
   "metadata": {},
   "source": [
    "Now that the loss is implemented, we need a way to update our parameters. For that we will perform the optimization procedure called `gradient descent`. It's a procedure by wich we compute the gradient of the loss with respect to each parameter, and then minimize it by steping in the descent direction.\n",
    "\n",
    "To compute the gradient of the loss with respect to each parameter, we will use an algorithm called `backpropagation` using the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cc83d9",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c234507",
   "metadata": {},
   "source": [
    "After the forward pass, we compute the loss $\\mathcal{L}$ of our prediction by comparing it to the true label. We then need to perform a gradient descent step to update our weight. For that we need to compute the gradient of $\\mathcal{L}$ with respect to each parameter.\n",
    "\n",
    "The loss is given by : $$ L(\\hat{y}, y) = \\sum_{c=1}^C - y_c log(s_c) $$\n",
    "\n",
    "Where $$ s_c = softmax(z_c) $$ \n",
    "\n",
    "We first need to compute $$ \\frac{d\\mathcal{L}}{dz} = s - y$$\n",
    "With $y$ being the one hot encoding of our true label.\n",
    "\n",
    "And then we use the chain rule to compute the gradient of the output layer weight $W_o$: \n",
    "$$ \\frac{d\\mathcal{L}}{dW_o} = \\frac{d\\mathcal{L}}{dz} \\frac{dz}{dW_o} $$\n",
    "\n",
    "and for the bias \n",
    "\n",
    "$$ \\frac{d\\mathcal{L}}{db} = \\frac{d\\mathcal{L}}{dz} \\frac{dz}{db} $$\n",
    "\n",
    "Which for a linear layer, since $z = W x + b$ : \n",
    "$$\\frac{dz}{dW} = x^T $$\n",
    "$$ \\frac{dz}{db} = 1$$\n",
    "\n",
    "We also need to compute the derivative of the output with respect to its input, so that we can keep going backwards through the network. With $z = W x + b$ :\n",
    "$$ \\frac{dz}{dx} = W^T $$\n",
    "\n",
    "We can thus keep unrolling the chain rule and backpropagate through the network. We also need to deal with the activation layers. If an activation layer produces output $y = \\sigma(z)$ we have :\n",
    "$$ \\frac{dy}{dz} = \\sigma'(z) = \\sigma(z)(1-\\sigma(z)) $$\n",
    "\n",
    "\n",
    "You now have all the formulaes to backpropagate the gradient of the loss through the network. More details in this [book](http://neuralnetworksanddeeplearning.com/) and in the course material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e099c3",
   "metadata": {},
   "source": [
    "TODO : prove that $$ \\forall c, \\frac{d\\mathcal{L}}{dz_c} = s_c - y_c$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2070e207",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : prove the expression above (pen and paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ab71d3",
   "metadata": {},
   "source": [
    "Your task is now to implement the backpropagation algorithm, we provide an API reference that you can use, but you are free to come up with your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f540c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: extend the classes before to implement the backward function\n",
    "### IMPORTANT : this is just API reference, you need to actually copy/paste the code for the forward propagation here\n",
    "### or you can just implement everything here\n",
    "\n",
    "class LinearLayer :\n",
    "    def backward(self, gradient) : \n",
    "        \"\"\"Computes the necessary gradients and output the gradient of the loss with respect to the input of this layer.\n",
    "        Hint : you might need to save some important value during forward propagation.\"\"\"\n",
    "        return gradient\n",
    "\n",
    "class SigmoidLayer : \n",
    "    def backward(self, gradient):\n",
    "        \"\"\"Computes the necessary gradients and output the gradient of the loss with respect to the input of this layer.\n",
    "        Hint : you might need to save some important value during forward propagation.\"\"\"\n",
    "        return gradient\n",
    "\n",
    "class MLP :\n",
    "    def backward(self, loss_gradient) : \n",
    "        \"\"\"Takes as input the gradient of the loss and performs backpropagation through each layer\"\"\"\n",
    "        return loss_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58b429e",
   "metadata": {},
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400815ad",
   "metadata": {},
   "source": [
    "Once the gradient $ \\nabla \\mathcal{L} $ has been computed, we can now update our parameters with a gradient descent step : for each parameter $\\theta$ : \n",
    "\n",
    "$$ \\theta \\leftarrow \\theta - \\alpha \\frac{d\\mathcal{L}}{d\\theta} $$\n",
    "\n",
    "Where $\\alpha$ is the learning rate (see previous lab).\n",
    "\n",
    "You can now implement the gradient step into the MLP, we also provide an API reference, but it's up to you if you want to change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2237e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: extend the classes before to implement the gradient step\n",
    "### IMPORTANT : this is just API reference, you need to actually copy/paste the code for the forward propagation here\n",
    "### or you can just implement everything here\n",
    "\n",
    "class LinearLayer :\n",
    "    def step(self, alpha) : \n",
    "        \"\"\"Performs the gradient step to update the parameters.\n",
    "        Hint : did you save the necessary gradients somewhere during backward ?\"\"\"\n",
    "        pass\n",
    "\n",
    "class SigmoidLayer : \n",
    "    def step(self, alpha):\n",
    "        \"\"\"Performs the gradient step to update the parameters.\n",
    "        Hint : does this function has parameters that need updating ?\"\"\"\n",
    "        pass\n",
    "\n",
    "class MLP :\n",
    "    def step(self, alpha) : \n",
    "        \"\"\"Perform the step function on each layer\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452da6dc",
   "metadata": {},
   "source": [
    "## Training the model.\n",
    "\n",
    "We will now train the model on the train set. You need to implement the training loop. You can use the previous lab's implementation as inspiration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3110d907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : implement the training loop\n",
    "def train(model, train_data, train_labels, lr, num_epochs):\n",
    "    ### Loop over epochs\n",
    "    for epoch in num_epochs : \n",
    "        pass\n",
    "        ### TODO : you need to : \n",
    "        ### - Loop over your training data\n",
    "        ### - setup the data as input / label\n",
    "        ### - forward pass your input through your model\n",
    "        ### - compute the loss between you prediction and the label\n",
    "        ### - compute the gradient of the loss \n",
    "        ### - take a gradient descent step to update the model's parameters using the learning rate lr\n",
    "        ### add some prints and save your loss so you can plot it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddd6e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : run the training of your model, you have more hyperparameters to choose.\n",
    "\n",
    "## Network parameters\n",
    "network_layers = [20,10] # description of your layers\n",
    "input_features = 64\n",
    "\n",
    "## Optimization parameters\n",
    "lr = 0.1\n",
    "num_epochs = 300\n",
    "\n",
    "## TODO:training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa97f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : plot the training loss over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a7cccc",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3c570b",
   "metadata": {},
   "source": [
    "We now need to test the model on the test set. For that we will compute different metrics : \n",
    "\n",
    "- The value of the loss on the test set.\n",
    "- The accuracy of the model, which is the rate of correct predictions.\n",
    "- The recall of the model is the rate of correct prediction over all the predictions of a specific class.\n",
    "\n",
    "See more details about accuracy and recall [here](https://en.wikipedia.org/wiki/Precision_and_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cf8bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : compute the loss on the test set, compute the accuracy, recall of your model. Optionnal : show the confusion matrix\n",
    "def test(model, test_set) :\n",
    "    return 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
