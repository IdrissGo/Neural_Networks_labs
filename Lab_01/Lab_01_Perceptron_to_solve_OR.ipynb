{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc1a2852",
   "metadata": {},
   "source": [
    "# Perceptron for the OR problem in NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b391f5",
   "metadata": {},
   "source": [
    "We will start by building and training a perceptron that will act as an OR gate.\n",
    "\n",
    "An OR gate is a simple system with two binary inputs and one output. It returns the logical OR of the inputs. That is :\n",
    "\n",
    "| x_1 | x_2 | out\n",
    "| --- | --- | --- |\n",
    "|  0  |  0  |  0  |\n",
    "|  0  |  1  |  1  |\n",
    "|  1  |  0  |  1  |\n",
    "|  1  |  1  |  1  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bf1a57",
   "metadata": {},
   "source": [
    "## Requirements \n",
    "\n",
    "We will first install and import the necessary requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf16b704",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae19940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## set random seed for reproducibility \n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a15589",
   "metadata": {},
   "source": [
    "A machine learning problem has three components : a dataset, a model, and an optimization method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d5b4cf",
   "metadata": {},
   "source": [
    "## Training data.\n",
    "\n",
    "We will first create the training data. A training datapoint has the form (X, Y) where X is the input data (in our case, the 2 values we need to OR) and Y is the label vector defining which output our model should give (in our case the label is 0 or 1).\n",
    "\n",
    "The dataset is the list of training data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61ac6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "X = np.array([\n",
    "    [0, 0],  \n",
    "    [0, 1],  \n",
    "    [1, 0],  \n",
    "    [1, 1] \n",
    "])\n",
    "Y = np.array([0,1,1,1])\n",
    "\n",
    "print(f\"The output of an OR gate if the input is {X[0]} is {Y[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02ca174",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "The model will be the function that will process our data and generate the required output. In our case the model will be a simple perceptron. \n",
    "\n",
    "A perceptron is a processing unit that works in a MISO (Multiple Input, Single Output) way. It computes the weighted sum of its inputs and adds a bias. Then the result is passed through an activation function to bring a non linearity.\n",
    "\n",
    "In other word for an input vectors $ \\textbf{x} = (x_0, x_1)$ a perceptron $u$ computes $u(\\textbf{x}) = \\sigma(w_0*x_0 + w_1*x_1 + b)$ where $\\sigma$ is the activation function, $w_0,w_1$ are the weights and $b$ is the bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036b5264",
   "metadata": {},
   "source": [
    "### Activation function\n",
    "\n",
    "In our case, the activation function will be the sigmoid function. \n",
    "$$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821d0ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : implement the sigmoid function\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x) :\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f5187b",
   "metadata": {},
   "source": [
    "You can plot the sigmoid function using matplotlib to validate your implementation. It should look like this :\n",
    " \n",
    "<img src=\"sigmoid.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbd04a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.arange(-10, 10, 0.1), sigmoid(np.arange(-10, 10, 0.1)), label=\"sigmoid\")\n",
    "plt.grid()\n",
    "plt.title(\"Sigmoid activation function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44ba87d",
   "metadata": {},
   "source": [
    "### The perceptron class\n",
    "\n",
    "Now we need to define the perceptron class as stated above. It should be able to take multiple input and be called to provide the output.\n",
    "\n",
    "Your work is to implement the class and its functionnalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21339bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the perceptron class using numpy\n",
    "\n",
    "class Perceptron : \n",
    "\n",
    "    def __init__(self, num_inputs):\n",
    "        ## TODO: init the perceptron class with the weights and bias, given the number of input it takes\n",
    "        pass\n",
    "\n",
    "    def __call__(self, x): ## this is required to be able to call the instance like a function\n",
    "        ## TODO : implement the perceptron function\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ccec94",
   "metadata": {},
   "source": [
    "Once the class has been created, you need now to instantiate a perceptron that can take 2 inputs, and test it on your dataset. It has not been trained so it will provide rubish results. The next section will be about the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a997f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : instantiate your model and test it on your training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89233f2d",
   "metadata": {},
   "source": [
    "## Training the model with gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0580e5",
   "metadata": {},
   "source": [
    "Now that the model is implemented, we need a training procedure to be able to learn the problem. We will use gradient descent to train the model parameters.\n",
    "\n",
    "In a few words, gradient descent consists in computing the gradient of the loss with respect to the parameters, and taking a step in the opposite direction, in order to minimize the loss. A more detailed explanation can be found in the course and [here](https://medium.com/@datasciencewizards/a-simple-guide-to-gradient-descent-algorithm-60cbb66a0df9)\n",
    "\n",
    "Our perceptron has three parameters :\n",
    "\n",
    "    - w_1 is the weight for the first input.\n",
    "\n",
    "    - w_2 is the weight for the second input.\n",
    "\n",
    "    - b is the bias of the perceptron.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6f298a",
   "metadata": {},
   "source": [
    "### Loss function and its gradient\n",
    "\n",
    "We define the L2-loss function between the prediction $ \\hat{y} $ and the label $ y $ as:\n",
    "\n",
    "$$\n",
    "L(\\hat{y},y) = (\\hat{y} - y)^2\n",
    "$$\n",
    "\n",
    "The prediction $ \\hat{y} $ for input data $ \\mathbf{x} = [x_0,x_1] $ is a function of the perceptron parameters (weights $ \\mathbf{w} = [w_0, w_1] $) and the bias $ b $, along with the sigmoid activation function $ \\sigma(z) = \\frac{1}{1 + e^{-z}} $:\n",
    "\n",
    "$$\n",
    "\n",
    "\\hat{y} = \\sigma(f) = \\frac{1}{1 + e^{-f}} \\\\\n",
    "~\\\\\n",
    "f(\\mathbf{x},\\mathbf{w},b) = w_0 \\cdot x_0 + w_1 \\cdot x_1 + b\n",
    "$$\n",
    "\n",
    "The derivative of the sigmoid function is:\n",
    "$$\n",
    "\\frac{\\partial \\sigma}{\\partial z}(z) = \\sigma(z) \\cdot (1 - \\sigma(z))\n",
    "$$\n",
    "\n",
    "### Derivatives for Gradient Descent\n",
    "\n",
    "To perform gradient descent, we need to compute the gradient of the loss function with respect to all parameters: $ w_0 $, $ w_1 $, and $ b $. The loss function is a composite function so we need to apply the chain rule. \n",
    "\n",
    "1. **Derivative with respect to $ w_0 $:**\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial w_0} = \\frac{\\partial L}{\\partial {\\hat{y}}} \\cdot \\frac{\\partial {\\hat{y}}}{\\partial f} \\cdot \\frac{\\partial {f}}{\\partial w_0}\n",
    "   $$\n",
    "\n",
    "   We have:\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial \\hat{y}} = 2 (\\hat{y} - y)   \\\\\n",
    "   ~\\\\\n",
    "   \\frac{\\partial \\hat{y}}{\\partial f} = \\hat{y} (1-\\hat{y}) \\\\\n",
    "   ~\\\\\n",
    "   \\frac{\\partial f}{\\partial w_0} = x_0\n",
    "   $$\n",
    "\n",
    "   Thus:\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial w_0} = 2(\\hat{y} - y) \\cdot \\hat{y}(1 - \\hat{y}) \\cdot x_0\n",
    "   $$\n",
    "\n",
    "2. **Derivative with respect to $ w_1 $:**\n",
    "\n",
    "   Similarly:\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial w_1} = 2(\\hat{y} - y) \\cdot \\hat{y}(1 - \\hat{y}) \\cdot x_1\n",
    "   $$\n",
    "\n",
    "3. **Derivative with respect to $ b $:**\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial b} = 2(\\hat{y} - y) \\cdot \\hat{y}(1 - \\hat{y})\n",
    "   $$\n",
    "\n",
    "### Parameter Update Rules\n",
    "\n",
    "The parameters are updated as follows:\n",
    "\n",
    "- $ w_0 \\leftarrow w_0 - \\alpha \\cdot \\frac{\\partial L}{\\partial w_0} $\n",
    "\n",
    "- $ w_1 \\leftarrow w_1 - \\alpha \\cdot \\frac{\\partial L}{\\partial w_1} $\n",
    "\n",
    "- $ b \\leftarrow b - \\alpha \\cdot \\frac{\\partial L}{\\partial b} $\n",
    "\n",
    "Where $ \\alpha $ is the learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a675b179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the L2 loss function\n",
    "def l2_loss(y_pred, y_true):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3539e945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : implement the training loop\n",
    "def train(model, X, Y, lr, num_epochs):\n",
    "    ### Loop over epochs\n",
    "    for epoch in num_epochs : \n",
    "        pass\n",
    "        ### TODO : you need to : \n",
    "        ### - Loop over your training data\n",
    "        ### - setup the data as input / label\n",
    "        ### - forward pass your input through your model\n",
    "        ### - compute the loss between you prediction and the label\n",
    "        ### - compute the gradient of the loss \n",
    "        ### - take a gradient descent step to update the model's parameters using the learning rate lr\n",
    "\n",
    "        ### optionnal : add some prints and save your loss so you can plot it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20df9f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: run the training on your model\n",
    "import numpy as np\n",
    "## instantiate model\n",
    "model = Perceptron()\n",
    "\n",
    "## select relevant hyperparameters \n",
    "lr = 0 # learning rate\n",
    "num_epochs = 0 # number of training epochs\n",
    "\n",
    "## run the training loop on your data \n",
    "train(model, X, Y, lr, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d3c7d5",
   "metadata": {},
   "source": [
    "### Visualization of the loss\n",
    "\n",
    "Create plots to show the loss over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce285ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : plot the loss over each epoch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb90451e",
   "metadata": {},
   "source": [
    "### Making a decision.\n",
    "\n",
    "Now that the model is trained, we can give it inputs to see what would be its decision. The output of our model is currently a value between 0 and 1. We want, however, the model to output exactly 0 or 1 depending on the value of the output. For that, if the value is greater than $0.5$, we will set the output to 1, and if the value is smaller than $0.5$, we set the value to 0. This is the decision step where we convert a probability to an actual choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0fe12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : Compute the decision of the model for each of the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1de0d7",
   "metadata": {},
   "source": [
    "## The XOR problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1db3ef",
   "metadata": {},
   "source": [
    "The XOR operation is the operation \"exclusive or\" where the output of the xor gate is 1 if and only if only one of the inputs is 1.\n",
    "\n",
    "The truthtable of the XOR operation is as follows : \n",
    "\n",
    "| x_1 | x_2 | out\n",
    "| --- | --- | --- |\n",
    "|  0  |  0  |  0  |\n",
    "|  0  |  1  |  1  |\n",
    "|  1  |  0  |  1  |\n",
    "|  1  |  1  |  0  |\n",
    "\n",
    "Train a perceptron on the new XOR problem, show that the results are not good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd15ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : Create the dataset of the XOR problem\n",
    "\n",
    "X = None\n",
    "Y = None\n",
    "\n",
    "## TODO : Instantiate a perceptron\n",
    "model = Perceptron()\n",
    "\n",
    "## TODO : Train the model\n",
    "train(model, X, Y, lr, num_epochs)\n",
    "\n",
    "## TODO : Show the decision of the model for each input \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
