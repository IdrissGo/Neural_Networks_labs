{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eca28ad1",
   "metadata": {},
   "source": [
    "# Implementing RNN for sequential data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266fa172",
   "metadata": {},
   "source": [
    "The previous labs were focused on image recognition. CNN are really good for that as they capture spatial information. But what happens when we have sequential data ? We need a model that is able to capture dependency through time or space, that's what RNNs are made for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e54d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup the imports \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775f7e03",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33404d7e",
   "metadata": {},
   "source": [
    "This time the dataset will be a weather dataset. You have 2 csv file (train and test data) with climate information over 4 years. Each data point has five features : the date, the temperature, the humidity, the pressure, and the wind speed. \n",
    "\n",
    "We will predict the meanpressure over time, but you can also select other targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff7c033",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First let's load the data\n",
    "with open(\"DailyDelhiClimateTrain.csv\") as f :\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)\n",
    "\n",
    "field_names = data[0]\n",
    "\n",
    "print(field_names)\n",
    "print(data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d013f066",
   "metadata": {},
   "source": [
    "As the data is in a csv file, loading it loads strings, we need to convert it to an input that the model is familiar with. The following code achieves this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961c8f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dates are in date format, we will simply increment a value and convert them to timestamp :\n",
    "timestamp = np.arange(len(data[1:]))\n",
    "print(timestamp)\n",
    "\n",
    "## the other values are float values that have been read as strings :\n",
    "meantemp = np.array([float(d[1]) for d in data[1:]])\n",
    "print(meantemp)\n",
    "\n",
    "humidity = np.array([float(d[2]) for d in data[1:]])\n",
    "print(humidity)\n",
    "\n",
    "windspeed = np.array([float(d[3]) for d in data[1:]])\n",
    "print(windspeed)\n",
    "\n",
    "meanpressure = np.array([float(d[4]) for d in data[1:]])\n",
    "print(meanpressure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca48b49",
   "metadata": {},
   "source": [
    "We will now create a dataset to load the data. We need to distinguish whether it's a train set or a test set. The following code creates a basic Climate Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a409e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClimateDataset :\n",
    "    def __init__(self, is_train, batch_size) -> None:\n",
    "        self.batch_size = batch_size\n",
    "        self.is_train = is_train\n",
    "        if self.is_train :\n",
    "            filename = \"DailyDelhiClimateTrain.csv\"\n",
    "        else : \n",
    "            filename = \"DailyDelhiClimateTest.csv\"\n",
    "        \n",
    "        with open(filename) as f :\n",
    "            reader = csv.reader(f)\n",
    "            data = list(reader)\n",
    "\n",
    "        self.field_names = data[0]\n",
    "\n",
    "        self.timestamp = np.arange(len(data[1:]))\n",
    "        meantemp = np.array([float(d[1]) for d in data[1:]])\n",
    "        humidity = np.array([float(d[2]) for d in data[1:]])\n",
    "        windspeed = np.array([float(d[3]) for d in data[1:]])\n",
    "        meanpressure = np.array([float(d[4]) for d in data[1:]])\n",
    "\n",
    "        ## we will predict the humidity\n",
    "        self.features = np.stack([\n",
    "            meantemp,\n",
    "            windspeed,\n",
    "            humidity\n",
    "        ]).T\n",
    "\n",
    "        self.target = meanpressure\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        return self.features\n",
    "    \n",
    "    def __len__(self): \n",
    "        return self.target.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        end = min(index + self.batch_size, len(self))\n",
    "        return self.features[index:end], self.target[index:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05593d8",
   "metadata": {},
   "source": [
    "Let's plot the data we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838ebceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ClimateDataset(True, 4)\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(14, 10)\n",
    "ax1 = fig.add_subplot(2, 2, 1)\n",
    "\n",
    "ax1.plot(range(len(ds)), ds.features[:,0])\n",
    "ax1.set_title(\"meantemp\")\n",
    "\n",
    "ax2 = fig.add_subplot(2, 2, 2)\n",
    "ax2.plot(range(len(ds)), ds.features[:,1])\n",
    "ax2.set_title(\"windspeed\")\n",
    "\n",
    "ax3 = fig.add_subplot(2, 2, 3)\n",
    "ax3.plot(range(len(ds)), ds.features[:,2])\n",
    "ax3.set_title(\"humidity\")\n",
    "\n",
    "ax4 = fig.add_subplot(2, 2, 4)\n",
    "ax4.plot(range(len(ds)), ds.target)\n",
    "ax4.set_title(\"meanpressure\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c9a845",
   "metadata": {},
   "source": [
    "As you can notice, the data is quite noisy, especially the mean pressure has outliers. This will be challenging for the network to learn. We need to preprocess the data to be able to have smoother inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb633e25",
   "metadata": {},
   "source": [
    "## Data preprocessing \n",
    "\n",
    "To preprocess the data, we will do three things : first we will deal with outliers, second we will run a sliding window to average over time and make a smoother input. Finally we will apply min-max processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea63f11c",
   "metadata": {},
   "source": [
    "### Outlier removal\n",
    "\n",
    "To deal with outliers in the pressure data, we will compute the mean value of the pressure and then look at every datapoint. If the difference between the pressure and the mean value is greater than a set threshold, then we assign the mean pressure to this datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501e4872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_outlier(pressure, threshold=50):\n",
    "   ### TODO : fix the outlier present in the pressure by assigning the mean pressure to data points that have irregular values.\n",
    "    return pressure\n",
    "\n",
    "## TODO : modify the ClimateDataset class to solve the outlier pressure problem during loading of the data.\n",
    "\n",
    "## TODO : plot the graph of the new pressure, it should look like the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc69c7f",
   "metadata": {},
   "source": [
    "Your new pressure should look like this : \n",
    "\n",
    "<img src=\"pressure_after_outlier_removal.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e113a24a",
   "metadata": {},
   "source": [
    "### Rolling average \n",
    "\n",
    "Now we need to smoothen our data, the data is very noisy and can easily confuse the network. To smoothen it we will run a rolling average (sliding window) over the data and compute the new datapoint at the average over the window. Rolling average is widely used when dealing with sequential or temporal data, as it highlights patterns and meaningful signals by smoothing out the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d967f74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_average(feat, window_size=30): \n",
    "    ## TODO : implement the rolling average.\n",
    "    ## new_feat[i] should be the mean of the window of size window_size that comes before i\n",
    "    ## if i < window_size then just take the mean from the beginning.\n",
    "    new_feat = np.zeros_like(feat)\n",
    "    \n",
    "    return new_feat\n",
    "\n",
    "\n",
    "## TODO : modify your dataset so that the features and target are smoothed with a window of size 30\n",
    "\n",
    "## TODO : plot your results, it should look like the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b9c284",
   "metadata": {},
   "source": [
    "Your data after rolling average should look like this : \n",
    "\n",
    "<img src=\"roll_averaged_data.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd839dc",
   "metadata": {},
   "source": [
    "### Min-max processing\n",
    "\n",
    "The last preprocessing that will be done will be min-max preprocessing.\n",
    "This time we are not dealing with image pixels but with recorded values of real world value. As you may notice, these values vary greatly in scale. Some are between 0 and 10 while others are in the thousand. We have seen in previous labs that neural networks are sensitive to scale, we thus need to normalize values between 0 and 1. We will use as we did before the min-max scaling : \n",
    "\n",
    "$$ x' = \\frac{x - min(x)}{max(x)-min(x)} $$\n",
    "\n",
    "However this time we have a `regression` problem and not a classification problem, so we need to also scale the `target`. This also means that you need to keep track of the $max$ and $min$ so you can retrieve the true values later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0362c927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_processing(feat) :\n",
    "    ## TODO : implement min-max preprocessing to scale the values between 0 and 1\n",
    "    ## ALSO return the min/max values so you can scale it back later\n",
    "\n",
    "    return feat\n",
    "\n",
    "## TODO : modify your dataset to apply min-max processing.\n",
    "\n",
    "    \n",
    "## TODO : plot the new values, should look like the following"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b85c9a",
   "metadata": {},
   "source": [
    "Your fully processed data should look like this, notice it's the same graph as before but with values between 0 and 1. \n",
    "\n",
    "<img src=\"fully_processed.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b6b5b3",
   "metadata": {},
   "source": [
    "### Train-test-val split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276ac63b",
   "metadata": {},
   "source": [
    "This time also we need train-test-validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed11323",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : create a train-test-val split for this task. \n",
    "## The data is already split into train-test. We need a validation set.\n",
    "## BE CAREFUL : you need the validation set to be SEQUENCES, DON'T SHUFFLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3c09b1",
   "metadata": {},
   "source": [
    "## Model implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f85bb1",
   "metadata": {},
   "source": [
    "We will implement the LSTM (long short term memory) module. LSTM is a modification of RNN that was introduced to deal with the problem of vanishing gradients in RNNs.\n",
    "\n",
    "The following is the diagram of the LSTM unit taken from [wikipedia](https://en.wikipedia.org/wiki/Long_short-term_memory)\n",
    "\n",
    "<img src=\"LSTM_Cell.svg\">\n",
    "\n",
    "\n",
    "An LSTM unit is composed of a cell and three gates : an input gate, an output gate, and a forget gate. The computation made by the unit are as follows : \n",
    "\n",
    "- $f_t = \\sigma_g(W_fx_t + U_fh_{t-1} + b_f)$   forget gate computation.\n",
    "- $i_t = \\sigma_g(W_ix_t + U_ih_{t-1} + b_o)$   input gate computation.\n",
    "- $o_t = \\sigma_g(W_ox_t + U_oh_{t-1} + b_o)$   output gate computation.\n",
    "- $c'_t = \\sigma_c(W_{c'}x_t + U_{c'}h_{t-1} + b_{c'})$  cell intermediate value.\n",
    "- $c_t = f_t\\odot c_{t-1} + i_t \\odot c'_t$    update of cell value.\n",
    "- $h_t = o_t \\odot \\sigma_h(c_t)$   update of hidden state value.\n",
    "\n",
    "Where $c_t$ represents the value of the cell at time $t$ and $h_t$ is the hidden state at time t, with $c_{-1}$ = 0 and $h_{-1}=0$. \n",
    "\n",
    "If we note $d$ the input dimension and $h$ the hidden dimension, $W \\in \\mathbb{R}^{h\\times d}$ and $U \\in \\mathbb{R}^{h\\times h}$ are weight matrices.\n",
    "\n",
    "Finally $\\sigma_g$ is the sigmoid activation function, $\\sigma_c$ is the hyperbolic tangent activation, and $\\sigma_h$ is either hyperbolic tangent or identity (you can choose).\n",
    "\n",
    "$\\odot$ is just the element-wise product.\n",
    "\n",
    "The hyperbolic tangent is given by : \n",
    "\n",
    "$$ tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a4868a",
   "metadata": {},
   "source": [
    "TODO : compute the derivative of the tanh function (pen and paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeb137b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : do the todo above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45b3db7",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "\n",
    "We will warm up by implementing the activation layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da633542",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : implement sigmoid, tanh, relu and their backward function. \n",
    "## normally you already have sigmoid and relu.\n",
    "## you can use np.thanh\n",
    "def sigmoid(x):\n",
    "    return x\n",
    "\n",
    "def tanh(x):\n",
    "    return x\n",
    "\n",
    "def relu(x):\n",
    "    return x\n",
    "\n",
    "class SigmoidLayer : \n",
    "    \"\"\"Use previous lab implementation\"\"\"\n",
    "\n",
    "class ReLULayer : \n",
    "    \"\"\"Use previous lab implementation\"\"\"\n",
    "\n",
    "class TanhLayer :\n",
    "    \"\"\"Implement the tanh layer similarly to the other layers\"\"\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7f7f2f",
   "metadata": {},
   "source": [
    "## Time dependence\n",
    "\n",
    "The model we are about to implement introduces a dependency through time. Since we are dealing with sequential data, the previous input of the sequence will influence the current processed input. This means that the same input could produce a different output, depending on the previous inputs. This is important to keep in mind because it means that backpropagation will not only consider the current gradient, but also gradients from previous computation. We are going to perform a BackPropagation Through Time (BPTT).\n",
    "\n",
    "### BPTT \n",
    "\n",
    "As a consequence of the dependence through time and the chain rule, gradients need to be accumulated over timesteps. Suppose we have a sequence of $T$ timesteps and want to backpropagate the gradient through them. The total loss of the sequence is : \n",
    "$$ \\mathcal{L}= \\sum_{t=1}^T \\mathcal{L_t} $$\n",
    "\n",
    "So the total gradient of the loss with respect to a parameter $ W$ will be : \n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial W} = \\sum_{t=1}^T \\frac{\\partial \\mathcal{L_t}} {\\partial W} $$\n",
    "\n",
    "For recurrent models like LSTMs, parameters $W$ influence the loss $\\mathcal{L}_t$ not just directly at timestep $t$, but also indirectly through their effect on earlier timesteps. This happens because the hidden state $h_t$, which depends on $W$, is passed forward through time and influences future states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec3f74a",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "For more details and information on how to perform backpropgation through time, see the supplementary information document present with this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc6b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gate:\n",
    "    \"\"\"Implement the gates.\n",
    "    Hint : have you noticed the gates look like linear layers and an activation ?\"\"\"\n",
    "\n",
    "    def __init__(self, input_channel, hidden_dim, activation_layer) -> None:\n",
    "        \"\"\"Should have a W matrix, a U matrix and a bias vector.\n",
    "        The activation layer is either a sigmoid or a tanh layer.\"\"\"\n",
    "\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        \"\"\"Performs gate computation, linear part and activation.\n",
    "           x is input at time t, h is hidden state at time t-1 \"\"\"\n",
    "        return h\n",
    "\n",
    "    def backward(self, dLdy, x_prev, h_prev): \n",
    "        \"\"\"Inputs : \n",
    "            - dLdy : gradient with respect to the output at time t.\n",
    "            - x_prev : input at time t\n",
    "            - h_prev : hidden state at time t-1.\n",
    "        \n",
    "        Computes : \n",
    "            dLdz : backward through the activation\n",
    "            dLdW : gradient with respect to W at time t\n",
    "            dLdU : gradient with respect to U at time t\n",
    "            dLdb : gradient with respect to b at time t \n",
    "        \n",
    "        The gradients should be SUMMED while you are considering the same sequence.\n",
    "\n",
    "        Outputs : \n",
    "            - dLdx_prev : gradient with respect to the input at time t\n",
    "            - dLdh_prev : gradient with respect to the hidden state at time t-1\"\"\"\n",
    "\n",
    "        dLdx_prev = np.zeros_like(x_prev)\n",
    "        dLdh_prev = np.zeros_like(h_prev)\n",
    "\n",
    "\n",
    "        return dLdx_prev, dLdh_prev\n",
    "    \n",
    "    def reset_gradients(self): \n",
    "        \"\"\"You should reset the gradients to 0 after step\"\"\"\n",
    "        \n",
    "    def step(self, lr) : \n",
    "        \"\"\"Update parameters and then reset gradients.\"\"\"\n",
    "\n",
    "        self.reset_gradients()\n",
    "    \n",
    "    def __call__(self, x, h):\n",
    "        return self.forward(x,h)\n",
    "        \n",
    "\n",
    "\n",
    "class LSTMLayer :\n",
    "    \"\"\"Implements the LSTM layer. You can follow the previous lab's API.\n",
    "    Use three gates and maintain a cell and a hidden state. Hint : the cell is also using a gate.\n",
    "    You should also maintain a cache while computing the same sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim) -> None:\n",
    "        \"\"\"Initialize gates and cell, initialize the cache of the sequence.\n",
    "        Note : this unit has no parameters, only the gates and cell have it.\"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"For a sequence you should : \n",
    "            - Reset the cache\n",
    "            - Find the length of the sequence.\n",
    "            - Loop through time and : \n",
    "                - Compute the output of each gate at time t.\n",
    "                - Compute the final cell value at time t.\n",
    "                - Compute the hidden state which is the output for the time t.\n",
    "                - Save ALL intermediate values to the cache\n",
    "            - Return the final output which should be a sequence ofthe same length as the input sequence.\"\"\"\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def backward(self, dLdh):\n",
    "        \"\"\"\n",
    "        Backward pass for the LSTM unit. \n",
    "        Input : \n",
    "            - dLdh : the gradient of the loss with respect to the output per time step.\n",
    "        \n",
    "        Output : \n",
    "            - dLdx : the gradient of the loss with respect to the input sequence.\n",
    "        \n",
    "        To correctly compute each gradient, you need to use the chain rule and the formulaes of the LSTM unit.\n",
    "        You will need to compute each intermediate value's gradient. Especially, before going backward in each gate,\n",
    "        you will need to compute the gradient w.r.t the output of each gate.\n",
    "        \"\"\"\n",
    "        T = len(dLdh) ## length of the sequence\n",
    "\n",
    "        dx = np.zeros((T, self.input_dim), dtype=np.float32) \n",
    "\n",
    "        for i in reversed(range(T)): ### Going back through time\n",
    "            pass\n",
    "\n",
    "            # Gradients for cell state and output\n",
    "\n",
    "            \n",
    "            # Gradients of the gate outputs\n",
    "\n",
    "            \n",
    "            # Backpropagate through gates\n",
    "\n",
    "            \n",
    "            # Combine gradients from all gates\n",
    "\n",
    "            ## Be mindful that after the first iteration, the gradient with respect to the\n",
    "            ## hidden state and the cell now have a new dependency.\n",
    "            ## See supplementary information to find that dependency\n",
    "\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "    def reset_cache(self) : \n",
    "        \"\"\"Reset the cache\"\"\"\n",
    "        pass\n",
    "\n",
    "    def step(self, lr):\n",
    "        \"\"\"Step through each gate, this unit has no parameters.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aa1a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer :\n",
    "    \"\"\"Use your previous lab implementation.\n",
    "    TODO : modify it to be able to take batched input.\n",
    "    Instead of being a vector of size C, it should be an input of size (T,C),\n",
    "    Where T is the sequence length.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9166bc",
   "metadata": {},
   "source": [
    "We will now implement a full model. You need to use linear layers from the MLP lab as well as at least one LSTM unit.\n",
    "Don't forget the activation for the linear layers.\n",
    "\n",
    "We need to predict only one value so the output should only have one dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de41e191",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : implement a RNN model, that has Linear layers and LSTM units.\n",
    "def clip(value, clip_threshold=10): \n",
    "    \"\"\"You can use gradient clipping like for the CNN lab.\n",
    "    Although it is likely not useful here.\"\"\"\n",
    "    return value\n",
    "\n",
    "class RNNModel :\n",
    "    \"\"\"Implements a full RNN model, with at least one LSTM unit.\n",
    "    Take inspiration from the full MLP and the full CNN model.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, lstm_dim, mlp_description) -> None:\n",
    "        \"\"\"Initializes self.layers using the descriptions.\n",
    "        Start with the lstm unit and then the linear part. \n",
    "        Don't forget activations (relu) for the linear part.\"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        return gradient\n",
    "    \n",
    "    def step(self, alpha):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891026ef",
   "metadata": {},
   "source": [
    "## Training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b622dc8",
   "metadata": {},
   "source": [
    "We will now train our model. This time our problem is a regression problem, because we are trying to predict a real value. We will thus use the same loss function as we used in lab 1 - Perceptron to solve OR. The L2-loss function : \n",
    "\n",
    "$$ \\mathcal{L}(\\hat{y}, y) = \\frac{1}{2}(\\hat{y}-y)^2$$\n",
    "\n",
    "Whose derivative is : \n",
    "\n",
    "$$ \\frac{\\mathcal{dL}}{d\\hat{y}} = (\\hat{y} - y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235305ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO : implement the L2-loss, you can reuse lab 1 implementation\n",
    "def l2_loss(y_pred, y_true):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db471182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : implement the training loop with the validation loop\n",
    "def validation(model, val_dataset):\n",
    "    \"\"\"Reuse previous lab implementation.\n",
    "    Be mindful the loss has changed.\"\"\"\n",
    "\n",
    "def train(model, train_set, lr, num_epochs, val_set):\n",
    "    losses = []\n",
    "    best_model = None\n",
    "    best_model_loss = float(\"inf\")\n",
    "    ### TODO : reuse previous labs implementation\n",
    "    ### be mindful the loss (and its derivative) has changed\n",
    "    ### Don't forget the validation loop\n",
    "    return losses, best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffddb581",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : train your model. The architecture bellow should give you good results\n",
    "## If the implementation is correct. Any deeper architecture might lead to vanishing gradients\n",
    "## which will require more optimization out of the scope of this lab\n",
    "model = RNNModel(3, 100, [1]) \n",
    "lr = 0.001\n",
    "num_epochs = 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe61461e",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1194cd",
   "metadata": {},
   "source": [
    "This time it is not a classification problem, so there is no accuracy or recall to compute. Instead we will compute the Root Mean Squared Error (RMSE) of the model over the test set. And visualize the prediction against the true values.\n",
    "\n",
    "The RMSE is given by : \n",
    "$$ RMSE = \\sqrt{\\sum_{i_1}^n \\frac{(y_i - \\hat{y_i})^2}{n}} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fc8770",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: COmpute the RMSE of the model over the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144b624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Show the predicted values and the test values on the same graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aa94b9",
   "metadata": {},
   "source": [
    "BONUS : Repeat the experiment for each possible feature of the dataset, changing the target each time. Plot the results every time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
