{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eca28ad1",
   "metadata": {},
   "source": [
    "# Implementing RNN for sequential data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266fa172",
   "metadata": {},
   "source": [
    "The previous labs were focused on image recognition. CNN are really good for that as they capture spatial information. But what happens when we have sequential data ? We need a model that is able to capture dependency through time or space, that's what RNNs are made for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e54d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup the imports \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775f7e03",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33404d7e",
   "metadata": {},
   "source": [
    "This time the dataset will be a weather dataset. You have 2 csv file (train and test data) with climate information over 4 years. Each data point has five features : the date, the temperature, the humidity, the pressure, and the wind speed. \n",
    "\n",
    "You will select one of these features as the target that you need to predict (except the date) and use the other features as the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31dae67",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eff7c033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['date', 'meantemp', 'humidity', 'wind_speed', 'meanpressure']\n",
      "['2013-01-01', '10.0', '84.5', '0.0', '1015.6666666666666']\n"
     ]
    }
   ],
   "source": [
    "## First let's load the data\n",
    "with open(\"DailyDelhiClimateTrain.csv\") as f :\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)\n",
    "\n",
    "field_names = data[0]\n",
    "\n",
    "print(field_names)\n",
    "print(data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d013f066",
   "metadata": {},
   "source": [
    "As the data is in a csv file, loading it loads strings, we need to convert it to an input that the model is familiar with. The following code achieves this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "961c8f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    1    2 ... 1459 1460 1461]\n",
      "[10.          7.4         7.16666667 ... 14.0952381  15.05263158\n",
      " 10.        ]\n",
      "[ 84.5         92.          87.         ...  89.66666667  87.\n",
      " 100.        ]\n",
      "[0.         2.98       4.63333333 ... 6.26666667 7.325      0.        ]\n",
      "[1015.66666667 1017.8        1018.66666667 ... 1017.9047619  1016.1\n",
      " 1016.        ]\n"
     ]
    }
   ],
   "source": [
    "## dates are in date format, we will simply increment a value and convert them to timestamp :\n",
    "timestamp = np.arange(len(data[1:]))\n",
    "print(timestamp)\n",
    "\n",
    "## the other values are float values that have been read as strings :\n",
    "meantemp = np.array([float(d[1]) for d in data[1:]])\n",
    "print(meantemp)\n",
    "\n",
    "humidity = np.array([float(d[2]) for d in data[1:]])\n",
    "print(humidity)\n",
    "\n",
    "windspeed = np.array([float(d[3]) for d in data[1:]])\n",
    "print(windspeed)\n",
    "\n",
    "meanpressure = np.array([float(d[4]) for d in data[1:]])\n",
    "print(meanpressure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca48b49",
   "metadata": {},
   "source": [
    "The data for the train set has been loaded, we need now to create a dataset so that we can access the data conveniently. Write the class for ClimateDataset such that accessing ds[i] returns the input data and the target at timestamp i.\n",
    "\n",
    "The dataset must distinguish between train and test data.\n",
    "\n",
    "You can chose the target to be predicted.\n",
    "\n",
    "IMPORTANT : You must be able to get a `sequence` of data, not just a single datapoint. So this time you should implement batch loading, so that we can extract sequences of arbitrary sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a409e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClimateDataset :\n",
    "    def __init__(self, is_train) -> None:\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def target(self):\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e915a50",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7d5618",
   "metadata": {},
   "source": [
    "This Time we are not dealing with image pixels but with recorded values of real world value. As you may notice, these values vary greatly in scale. Some are between 0 and 10 while others are in the thousand. We have seen in previous labs that neural networks are sensitive to scale, we thus need to normalize values between 0 and 1. We will use as we did before the min-max scaling : \n",
    "\n",
    "$$ x' = \\frac{x - min(x)}{max(x)-min(x)} $$\n",
    "\n",
    "However this time we have a `regression` problem and not a classification problem, so we need to also scale the `target`. This also means that you need to keep track of the $max$ and $min$ so you can retrieve the true values later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2d9dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: Apply mean-max preprocessing to all the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b6b5b3",
   "metadata": {},
   "source": [
    "### Train-test-val split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276ac63b",
   "metadata": {},
   "source": [
    "This time also we need a validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bed11323",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : create a validation split for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3c09b1",
   "metadata": {},
   "source": [
    "## Model implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f85bb1",
   "metadata": {},
   "source": [
    "We will implement the LSTM (long short term memory) module. LSTM is a modification of RNN that was introduced to deal with the problem of vanishing gradients in RNNs.\n",
    "\n",
    "The following is the diagram of the LSTM unit taken from [wikipedia](https://en.wikipedia.org/wiki/Long_short-term_memory)\n",
    "\n",
    "<img src=\"LSTM_Cell.svg\">\n",
    "\n",
    "\n",
    "An LSTM unit is composed of a cell and three gates : an input gate, an output gate, and a forget gate. The computation made by the unit are as follows : \n",
    "\n",
    "- $f_t = \\sigma_g(W_fx_t + U_fh_{t-1} + b_f)$   forget gate computation.\n",
    "- $i_t = \\sigma_g(W_ix_t + U_ih_{t-1} + b_o)$   input gate computation.\n",
    "- $o_t = \\sigma_g(W_ox_t + U_oh_{t-1} + b_o)$   output gate computation.\n",
    "- $c'_t = \\sigma_c(W_{c'}x_t + U_{c'}h_{t-1} + b_{c'})$  cell intermediate value.\n",
    "- $c_t = f_t\\odot c'_{t-1} + i_t \\odot c'_t$    update of cell value.\n",
    "- $h_t = o_t \\odot \\sigma_h(c_t)$   update of hidden state value.\n",
    "\n",
    "Where $c_t$ represents the value of the cell at time $t$ and $h_t$Â is the hidden state at time t, with $c_0$ = 0 and $h_0=0$. \n",
    "\n",
    "If we note $d$ the input dimension and $h$ the hidden dimension, $W \\in \\mathbb{R}^{h\\times d}$ and $U \\in \\mathbb{R}^{h\\times h}$ are weight matrices.\n",
    "\n",
    "Finally $\\sigma_g$ is the sigmoid activation function, $\\sigma_c$ is the hyperbolic tangent activation, and $\\sigma_h$ is either hyperbolic tangent or identity (you can choose).\n",
    "\n",
    "$\\odot$ is just the element-wise product.\n",
    "\n",
    "The hyperbolic tangent is given by : \n",
    "\n",
    "$$ tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a4868a",
   "metadata": {},
   "source": [
    "TODO : compute the derivative of the tanh function (pen and paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da633542",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : implement sigmoid and tanh and their backward function. \n",
    "## normally you already have sigmoid.\n",
    "def sigmoid(x):\n",
    "    return 0\n",
    "\n",
    "def tanh(x):\n",
    "    return 0\n",
    "\n",
    "class SigmoidLayer : \n",
    "    \"\"\"This implements the Sigmoid layer\"\"\"\n",
    "\n",
    "class TanhLayer :\n",
    "    \"\"\"This implements the tanh layer\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a34c3e",
   "metadata": {},
   "source": [
    "We will now implement the LSTM unit. For that, it is good to separate it into gates, so you can flow the gradients properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc6b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gate:\n",
    "    \"\"\"Implement the gates.\n",
    "    Hint : have you noticed the gates perform the same computation, \n",
    "    only with a different set of weights?\"\"\"\n",
    "\n",
    "class LSTMLayer :\n",
    "    \"\"\"Implements the LSTM layer. You can follow the previous lab's API.\n",
    "    Use three gates and maintain a cell and a hidden state. \n",
    "    The output should be the hidden state at time t.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_state_dim) -> None:\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        return gradient\n",
    "    \n",
    "    def step(self, alpha):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9166bc",
   "metadata": {},
   "source": [
    "We will now implement a full model. You need to use linear layers from the MLP lab as well as at least one LSTM unit.\n",
    "Don't forget the activation for the linear layers.\n",
    "\n",
    "We need to predict only one value so the output should only have one dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de41e191",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : implement a RNN model, that has Linear layers and LSTM units.\n",
    "class RNNModel :\n",
    "    \"\"\"Implements a full RNN model, with at least one LSTM unit.\n",
    "    Take inspiration from the full MLP and the full CNN model.\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        return gradient\n",
    "    \n",
    "    def step(self, alpha):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891026ef",
   "metadata": {},
   "source": [
    "## Training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b622dc8",
   "metadata": {},
   "source": [
    "We will now train our model. This time our problem is a regression problem, because we are trying to predict a real value. We will thus use the same loss function as we used in lab 1 - Perceptron to solve OR. The L2-loss function : \n",
    "\n",
    "$$ \\mathcal{L}(\\hat{y}, y) = (y-\\hat{y})^2 $$\n",
    "\n",
    "Whose derivative is : \n",
    "\n",
    "$$ \\frac{\\mathcal{L}}{d\\hat{y}} = 2(y-\\hat{y}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235305ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO : implement the L2-loss, you can reuse lab 1 implementation\n",
    "def l2_loss(y_pred, y_true):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db471182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : implement the training loop with the validation loop\n",
    "## IMPORTANT : this time your RNNN should deal with SEQUENTIAL data, and output MULTIPLE values\n",
    "def train(model, train_data, train_labels, validation_data, validation_labels, lr, num_epochs):\n",
    "    best_model = None  ## should be returned at the end of the training\n",
    "    best_model_validation_loss = None\n",
    "    ### Loop over epochs\n",
    "    for epoch in num_epochs : \n",
    "        pass\n",
    "        ### TODO : training loop, similar to previous lab\n",
    "\n",
    "        ### TODO : implement validation loop that runs every few epoch.\n",
    "        ### it should allow to choose the best model as the one that minimizes validation loss\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe61461e",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1194cd",
   "metadata": {},
   "source": [
    "This time it is not a classification problem, so there is no accuracy or recall to compute. Instead we will compute the Root Mean Squared Error (RMSE) of the model over the test set. And visualize the prediction against the true values.\n",
    "\n",
    "The RMSE is given by : \n",
    "$$ RMSE = \\sqrt{\\sum_{i_1}^n \\frac{(y_i - \\hat{y_i})^2}{n}} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fc8770",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: COmpute the RMSE of the model over the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144b624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Show the predicted values and the test values on the same graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aa94b9",
   "metadata": {},
   "source": [
    "BONUS : Repeat the experiment for each possible feature of the dataset, changing the target each time. Plot the results every time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
